# -*- coding: utf-8 -*-
"""Tugas Akhir SVM & LSTM-Development.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1olw-I05hX0V8YKnWoNJNUYwgprzEV58q

# Import Library & Function
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/My Drive/TUGAS AKHIR/Labeled

# Commented out IPython magic to ensure Python compatibility.
# DataFrame
import pandas as pd

# Matplot
import matplotlib.pyplot as plt
# %matplotlib inline

# Scikit-learn
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.model_selection import StratifiedKFold, cross_val_score

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.metrics import make_scorer, accuracy_score, f1_score
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score, recall_score, precision_score

from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.preprocessing import LabelEncoder
from sklearn.manifold import TSNE

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Keras
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM, SpatialDropout1D
from keras import utils
from keras.callbacks import ReduceLROnPlateau, EarlyStopping

# nltk
import nltk
from nltk.corpus import stopwords

# Word2vec
import gensim

# Utility
import numpy as np
import os
from collections import Counter
import logging
import time
import pickle
import itertools
import requests

"""## Stemming function"""

!pip install Sastrawi #Indonesian word stemmer
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
# create stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()

"""## Remove Punctuation Function"""

import string
string.punctuation
def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree

"""## Tokenization function"""

import re
def tokenization(text):
    return nltk.word_tokenize(text)

"""## Stopword & NLP library"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

# CONSTRUCT STOPWORDS
rama_stopword = "https://raw.githubusercontent.com/ramaprakoso/analisis-sentimen/master/kamus/stopword.txt"
yutomo_stopword = "https://raw.githubusercontent.com/yasirutomo/python-sentianalysis-id/master/data/feature_list/stopwordsID.txt"
fpmipa_stopword = "https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-list/fpmipa-stopwords.txt"
sastrawi_stopword = "https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-list/sastrawi-stopwords.txt"
aliakbar_stopword = "https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-list/aliakbars-bilp.txt"
pebahasa_stopword = "https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-list/pebbie-pebahasa.txt"
elang_stopword = "https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-id.txt"
nltk_stopword = stopwords.words('indonesian')

# create path url for each stopword
path_stopwords = [rama_stopword, yutomo_stopword, fpmipa_stopword, sastrawi_stopword,
                  aliakbar_stopword, pebahasa_stopword, elang_stopword]

# combine stopwords
stopwords_l = nltk_stopword
for path in path_stopwords:
    response = requests.get(path)
    stopwords_l += response.text.split('\n')

#Slang word
slang = '''
yg yang dgn ane smpai bgt gua gwa si tu ama utk udh btw
ntar lol ttg emg aj aja tll sy sih kalo nya trsa mnrt nih
ma dr ajaa tp akan bs bikin kta pas pdahl bnyak guys abis tnx
bang banget nang mas amat bangettt tjoy hemm haha sllu hrs lanjut
bgtu sbnrnya trjadi bgtu pdhl sm plg skrg
'''

# create dictionary with unique stopword
st_words = set(stopwords_l)
slang_word = set(slang.split())

# result stopwords
stop_words = st_words | slang_word
print(f'Stopwords: {list(stop_words)[:5]}')
print(len(stop_words))

def remove_stopwords(text):
    output= [i for i in text if i not in stop_words]
    return output

"""# Data Preprocessing"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/TUGAS AKHIR/

data_shopee = pd.read_csv('dataset_shopee_cleaned.csv')
data_lazada = pd.read_csv('dataset_lazada_cleaned.csv')
data_tokopedia = pd.read_csv('dataset_tokopedia_cleaned.csv')
data_bukalapak = pd.read_csv('dataset_bukalapak_cleaned.csv')
print("===========SHOPEE=============")
print(data_shopee.isna().sum())
print("===========LAZADA=============")
print(data_lazada.isna().sum())
print("==========TOKOPEDIA===========")
print(data_tokopedia.isna().sum())
print("==========BUKALAPAK===========")
print(data_bukalapak.isna().sum())

"""## Drop Null"""

data_shopee.dropna(inplace=True)
data_lazada.dropna(inplace=True)
data_tokopedia.dropna(inplace=True)
data_bukalapak.dropna(inplace=True)
print("============AFTER=============")
print("==============================")
print("===========SHOPEE=============")
print(data_shopee.isna().sum())
print("===========LAZADA=============")
print(data_lazada.isna().sum())
print("==========TOKOPEDIA===========")
print(data_tokopedia.isna().sum())
print("==========BUKALAPAK===========")
print(data_bukalapak.isna().sum())

"""## Changing Slang Word

Merubah kata singkatan atau merubah kata menjadi bentuk baku

"""

# Commented out IPython magic to ensure Python compatibility.
def remove_slangs(data) :
#   %cd /content/drive/My Drive/TUGAS AKHIR/data tambahan
  slangs1 = pd.read_csv('slang.txt', delimiter='=')
  j=0
  for index, row in slangs1.iterrows():
    abv = row[0].lower()
    data['text'] = [x.replace(abv, row[1]) for x in data['text']]
  j = j + 1

#slangs2.drop(['In-dictionary', 'context', 'category1', 'category2', 'category3'], axis=1, inplace=True)
slangs2.head()

def change_slangs(data) :
  slangs1 = pd.read_csv('slangs_simplified.csv', delimiter=',', nrows=5000)
  slangs2 = pd.read_csv('slangs_simplified.csv', delimiter=',', skiprows=5000, nrows=5000)
  slangs3 = pd.read_csv('slangs_simplified.csv', delimiter=',', skiprows=10000)

  j= 0
  for index, row in slangs1.iterrows():
    abv = row[1].lower()
    data = [x.replace(abv, row[2]) for x in data]

    j = j + 1


  j= 0
  for index, row in slangs2.iterrows():
    abv = row[1].lower()
    data = [x.replace(abv, row[2]) for x in data]

    j = j + 1


  j= 0
  for index, row in slangs3.iterrows():
    abv = row[1].lower()
    data = [x.replace(abv, row[2]) for x in data]

    j = j + 1

data = pd.read_csv('dataset_shopee_cleaned.csv', delimiter=',',nrows=1000)

slangs1 = pd.read_csv('slangs_simplified.csv', delimiter=',', nrows=5000)

j= 0
for index, row in slangs1.iterrows():
  abv = row[1]
  data['msg_lower'] = [x.replace(abv, row[2]) for x in data['msg_lower']]

  j = j + 1

#change_slangs(data['msg_lower'])

data.head()

"""## Stemming"""

#data = pd.read_csv('dataset_shopee_cleaned.csv', nrows=500)

data['stemmed']= data['text'].apply(lambda x:stemmer.stem(x))
data.head(10)

"""## Removing Punctuation"""

data['clean_msg']= data['text'].apply(lambda x:remove_punctuation(x))
data.head()

"""## Lowercasing"""

data['msg_lower']= data['clean_msg'].apply(lambda x: x.lower())
data.head()

"""## Tokenization"""

#data['msg_tokenied']= data['msg_lower'].apply(lambda x: tokenization(x))
data['tokenized']= data.apply(lambda row: nltk.word_tokenize(row['msg_lower']), axis=1)
data.head()

"""## Stop word removal"""

data['no_stopwords']= data['tokenized'].apply(lambda x:remove_stopwords(x))
data.head()

"""# Data Balance Check

## Shopee
"""

data_shopee = pd.read_csv('dataset_shopee_labeled.csv')
label_shopee = Counter(data_shopee.label)
print(label_shopee.keys(),label_shopee.values())

plt.figure(figsize=(16,8))
plt.bar(label_shopee.keys(), label_shopee.values())
plt.title("Dataset labels distribuition Shopee")

# Class count
count_class_0, count_class_1 = data_shopee.label.value_counts()

# Divide by class
df_class_0 = data_shopee[data_shopee['label'] == 0]
df_class_1 = data_shopee[data_shopee['label'] == 1]

df_class_1_over = df_class_1.sample(count_class_0, replace=True)
df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)

print('Random over-sampling:')
print(df_test_over.label.value_counts())

df_test_over.label.value_counts().plot(kind='bar', title='Count (label)');

"""## Lazada"""

data_lazada = pd.read_csv('dataset_lazada_labeled.csv')
label_lazada = Counter(data_lazada.label)
print(label_lazada.keys(),label_lazada.values())

plt.figure(figsize=(16,8))
plt.bar(label_lazada.keys(), label_lazada.values())
plt.title("Dataset labels distribuition Lazada")

"""## Bukalapak"""

data_bukalapak = pd.read_csv('dataset_bukalapak_labeled.csv')
label_bukalapak = Counter(data_bukalapak.label)
print(label_bukalapak.keys(),label_bukalapak.values())

plt.figure(figsize=(8,6))
plt.bar(label_bukalapak.keys(), label_bukalapak.values())
plt.title("Dataset labels distribuition Bukalapak")

"""## Tokopedia"""

data_tokopedia = pd.read_csv('dataset_tokopedia_labeled.csv')
label_tokopedia = Counter(data_tokopedia.label)
print(label_tokopedia.keys(),label_tokopedia.values())

plt.figure(figsize=(16,8))
plt.bar(label_tokopedia.keys(), label_tokopedia.values())
plt.title("Dataset labels distribuition Tokopedia")

"""## Graph"""

fig, ax = plt.subplots(2, 2, figsize=(16,10))

label_shopee = Counter(data_shopee.label)

print(label_shopee.keys(),label_shopee.values())
print(label_lazada.keys(),label_lazada.values())
print(label_tokopedia.keys(),label_tokopedia.values())
print(label_bukalapak.keys(),label_bukalapak.values())

ax[0,0].bar(label_shopee.keys(), label_shopee.values())
ax[0,0].set_title("Dataset labels distribuition Shopee")


ax[0,1].bar(label_lazada.keys(), label_lazada.values())
ax[0,1].set_title("Dataset labels distribuition Lazada")

ax[1,0].bar(label_tokopedia.keys(), label_tokopedia.values())
ax[1,0].set_title("Dataset labels distribuition Tokopedia")

ax[1,1].bar(label_bukalapak.keys(), label_bukalapak.values())
ax[1,1].set_title("Dataset labels distribuition Bukalapak")

data = pd.read_csv('Dataset Labeled.csv')
label = Counter(data.label)
print(label.keys(),label.values())

plt.figure(figsize=(16,8))
plt.bar(label.keys(), label.values())
plt.title("Dataset labels distribuition Tokopedia")

"""# Main Function

SVM  : Tf-idf

LSTM

Word Embedding :
* Word2Vec
* Fasttext

## SVM

### Import Library & function

#### Word2Vec function 1 with n-gram (Depecrated/Error)
"""

vectorizer2 = CountVectorizer(
    analyzer = 'word',
    tokenizer = tokenization,
    #lowercase = True,
    ngram_range=(1, 1),
    stop_words = stopwords)

"""#### Word2Vec function 2 with TfidVectorizer

This one Works
"""

vectorizer = TfidfVectorizer(
    min_df = 5,
    max_df = 0.8,
    sublinear_tf = True,
    use_idf = True)

"""### Using Full Data"""

# Commented out IPython magic to ensure Python compatibility.
#DATA TESTING
# %cd /content/drive/My Drive/TUGAS AKHIR/
data_test_shopee = pd.read_csv('review_shopee.csv')
data_test_lazada = pd.read_csv('review_lazada.csv')
data_test_tokopedia = pd.read_csv('review_tokopedia.csv')
data_test_bukalapak = pd.read_csv('review_bukalapak.csv')

"""#### Shopee

##### Implementing
"""

#data_shopee = pd.read_csv('dataset_shopee_cleaned.csv', nrows=100000)
data_shopee[data_shopee.isnull().any(axis=1)]

data_shopee.dropna(inplace=True)
print(data_shopee.shape)
data_shopee.head()

data_shopee['clean_msg'] = data_shopee['text'].apply(lambda x:remove_punctuation(x))
data_shopee['stemmed']= data_shopee['clean_msg'].apply(lambda x:stemmer.stem(x))
data_shopee['msg_lower']= data_shopee['stemmed'].apply(lambda x: x.lower())
data_shopee['tokenized']= data_shopee.apply(lambda row: nltk.word_tokenize(row['msg_lower']), axis=1)
data_shopee['no_stopwords']= data_shopee['tokenized'].apply(lambda x:remove_stopwords(x))
data_shopee.head()

data_shopee.to_csv('dataset_shopee_processed.csv')

train_shopee, test_shopee = train_test_split(data_shopee[['text','label','msg_lower']], test_size=0.3, random_state=42)
print("TRAIN size:", len(train_shopee))
print("TEST size:", len(test_shopee))

vectorizer_shopee = TfidfVectorizer(
    min_df = 5,
    max_df = 0.8,
    sublinear_tf = True,
    use_idf = True)

train_vectors = vectorizer_shopee.fit_transform(train_shopee['msg_lower'].tolist())
test_vectors = vectorizer_shopee.transform(test_shopee['msg_lower'].tolist())

import time
from sklearn import svm
from sklearn.metrics import classification_report

# Classification with SVM, kernel=linear
classifier_linear = svm.SVC(kernel='linear')
t0 = time.time()
classifier_linear.fit(train_vectors, train_shopee['label'])
t1 = time.time()
prediction_linear = classifier_linear.predict(test_vectors)
t2 = time.time()
time_linear_train = t1-t0
time_linear_predict = t2-t1

# results
print("Training time: %fs; Prediction time: %fs" % (time_linear_train, time_linear_predict))
report = classification_report(test_shopee['label'], prediction_linear, output_dict=True)

print('positive: ', report['positif'])
print('negative: ', report['negatif'])

print("Training time: %fs; Prediction time: %fs" % (time_linear_train, time_linear_predict))

"""##### Testing

###### Testing pada Dataset lain | Label : Random
"""

# SHOPEE
from random import randrange
idx = randrange(len(data_test_shopee['content']))

review = data_test_shopee.loc[idx]
review_vector = vectorizer_shopee.transform([review['content']])
print('content : ',review['content'])
print('sentimen : ',classifier_linear.predict(review_vector))

# LAZADA
from random import randrange
idx = randrange(len(data_test_lazada['content']))

review = data_test_lazada.loc[idx]
review_vector = vectorizer_shopee.transform([review['content']])
print('content : ',review['content'])
print('sentimen : ',classifier_linear.predict(review_vector))

# BUKALAPAK
from random import randrange
idx = randrange(len(data_test_bukalapak['content']))

review = data_test_bukalapak.loc[idx]
review_vector = vectorizer_shopee.transform([review['content']])
print('content : ',review['content'])
print('sentimen : ',classifier_linear.predict(review_vector))

# TOKOPEDIA
from random import randrange
idx = randrange(len(data_test_tokopedia['content']))

review = data_test_tokopedia.loc[idx]
review_vector = vectorizer_shopee.transform([review['content']])
print('content : ',review['content'])
print('sentimen : ',classifier_linear.predict(review_vector))

"""##### Saving Models (Pickle)"""

import pickle
# pickling the vectorizer
pickle.dump(vectorizer, open('vectorizer_shopee.sav', 'wb'))
# pickling the model
pickle.dump(classifier_linear, open('classifier_shopee.sav', 'wb'))

"""#### Lazada

##### Implementing
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/TUGAS AKHIR/Labeled
data_lazada = pd.read_csv('dataset_lazada_labeled.csv')
data_lazada.dropna(inplace=True)
print(data_lazada.shape)


data_lazada.head()

data_lazada['clean_msg'] = data_lazada['text'].apply(lambda x:remove_punctuation(x))
data_lazada['stemmed']= data_lazada['clean_msg'].apply(lambda x:stemmer.stem(x))
data_lazada['msg_lower']= data_lazada['stemmed'].apply(lambda x: x.lower())
data_lazada['tokenized']= data_lazada.apply(lambda row: nltk.word_tokenize(row['msg_lower']), axis=1)
data_lazada['no_stopwords']= data_lazada['tokenized'].apply(lambda x:remove_stopwords(x))
data_lazada.head()

data_lazada.to_csv('dataset_lazada_processed.csv')

train_lazada, test_lazada = train_test_split(data_lazada[['text','label','msg_lower','tokenized','no_stopwords']], test_size=0.3, random_state=42)
train_lazada.head()

vectorizer_lazada = TfidfVectorizer(
    min_df = 5,
    max_df = 0.8,
    sublinear_tf = True,
    use_idf = True)

train_vectors = vectorizer_lazada.fit_transform(train_lazada['msg_lower'].tolist())
test_vectors = vectorizer_lazada.transform(test_lazada['msg_lower'].tolist())

import time
from sklearn import svm
from sklearn.metrics import classification_report

# Classification with SVM, kernel=linear
classifier_linear = svm.SVC(kernel='linear')
t0 = time.time()
classifier_linear.fit(train_vectors, train_lazada['label'])
t1 = time.time()
prediction_linear = classifier_linear.predict(test_vectors)
t2 = time.time()
time_linear_train = t1-t0
time_linear_predict = t2-t1

# results
print("Training time: %fs; Prediction time: %fs" % (time_linear_train, time_linear_predict))
report = classification_report(test_lazada['label'], prediction_linear, output_dict=True)

print('positive: ', report['positif'])
print('negative: ', report['negatif'])

print("Training time: %fs; Prediction time: %fs" % (time_linear_train, time_linear_predict))

"""##### Testing

###### Testing pada Dataset lain | Label : Random
"""

# SHOPEE
from random import randrange
idx = randrange(len(data_test_shopee['content']))

review = data_test_shopee.loc[idx]
review_vector = vectorizer_lazada.transform([review['content']])
print('text : ',review['content'])
print('sentimen : ',classifier_linear.predict(review_vector))

# LAZADA
from random import randrange
idx = randrange(len(data_test_lazada['content']))

review = data_test_lazada.loc[idx]
review_vector = vectorizer_lazada.transform([review['content']])
print('text : ',review['content'])
print('sentimen : ',classifier_linear.predict(review_vector))

# TOKOPEDIA
from random import randrange
idx = randrange(len(data_test_tokopedia['content']))

review = data_test_tokopedia.loc[idx]
review_vector = vectorizer_lazada.transform([review['content']])

print('text : ',review['content'])
print('.sentimen : ',classifier_linear.predict(review_vector))

# BUKALAPAK
from random import randrange
idx = randrange(len(data_test_bukalapak['content']))

review = data_test_bukalapak.loc[idx]
review_vector = vectorizer_lazada.transform([review['content']])

print('text : ',review['content'])
print('sentimen : ',classifier_linear.predict(review_vector))

"""##### Saving Models (Pickle)"""

import pickle
# pickling the vectorizer
pickle.dump(vectorizer_lazada, open('vectorizer_lazada.sav', 'wb'))
# pickling the model
pickle.dump(classifier_linear, open('classifier_lazada.sav', 'wb'))

"""#### Tokopedia

##### Implementing
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/TUGAS AKHIR/Labeled
data_tokopedia = pd.read_csv('dataset_tokopedia_labeled.csv')
print(data_tokopedia.shape)
data_tokopedia.head()

data_tokopedia['clean_msg'] = data_tokopedia['text'].apply(lambda x:remove_punctuation(x))
data_tokopedia['stemmed']= data_tokopedia['clean_msg'].apply(lambda x:stemmer.stem(x))
data_tokopedia['msg_lower']= data_tokopedia['stemmed'].apply(lambda x: x.lower())
data_tokopedia['tokenized']= data_tokopedia.apply(lambda row: nltk.word_tokenize(row['msg_lower']), axis=1)
data_tokopedia['no_stopwords']= data_tokopedia['tokenized'].apply(lambda x:remove_stopwords(x))
data_tokopedia.head()

data_tokopedia.to_csv('dataset_tokopedia_processed.csv')

train_tokopedia, test_tokopedia = train_test_split(data_tokopedia[['text','label','msg_lower','tokenized','no_stopwords']], test_size=0.3, random_state=42)
train_tokopedia.head()

vectorizer_tokopedia = TfidfVectorizer(
    min_df = 5,
    max_df = 0.8,
    sublinear_tf = True,
    use_idf = True)

train_vectors = vectorizer_tokopedia.fit_transform(train_tokopedia['msg_lower'].tolist())
test_vectors = vectorizer_tokopedia.transform(test_tokopedia['msg_lower'].tolist())

import time
from sklearn import svm
from sklearn.metrics import classification_report

# classification with SVM, kernel=linear
classifier_linear = svm.SVC(kernel='linear')
t0 = time.time()
classifier_linear.fit(train_vectors, train_tokopedia['label'])
t1 = time.time()
prediction_linear = classifier_linear.predict(test_vectors)
t2 = time.time()
time_linear_train = t1-t0
time_linear_predict = t2-t1

# results
print("Training time: %fs; Prediction time: %fs" % (time_linear_train, time_linear_predict))
report = classification_report(test_tokopedia['label'], prediction_linear, output_dict=True)

print('positive: ', report['positif'])
print('negative: ', report['negatif'])

print("Training time: %fs; Prediction time: %fs" % (time_linear_train, time_linear_predict))

"""##### Testing

###### Testing pada Dataset lain | Label : Random
"""

# SHOPEE
from random import randrange
idx = randrange(len(data_test_shopee['content']))

review = data_test_shopee.loc[idx]
review_vector = vectorizer_tokopedia.transform([review['content']])

print('text : ',review['content'])
print('.sentimen : ',classifier_linear.predict(review_vector))

# LAZADA
from random import randrange
idx = randrange(len(data_test_lazada['content']))

review = data_test_lazada.loc[idx]
review_vector = vectorizer_tokopedia.transform([review['content']])

print('text : ',review['content'])
print('.sentimen : ',classifier_linear.predict(review_vector))

# TOKOPEDIA
from random import randrange
idx = randrange(len(data_test_tokopedia['content']))

review = data_test_tokopedia.loc[idx]
review_vector = vectorizer_tokopedia.transform([review['content']])

print('text : ',review['content'])
print('.sentimen : ',classifier_linear.predict(review_vector))

# BUKALAPAK
from random import randrange
idx = randrange(len(data_test_bukalapak['content']))

review = data_test_bukalapak.loc[idx]
review_vector = vectorizer_tokopedia.transform([review['content']])

print('text : ',review['content'])
print('.sentimen : ',classifier_linear.predict(review_vector))

"""##### Saving Models (Pickle)"""

import pickle
# pickling the vectorizer
pickle.dump(vectorizer_tokopedia, open('vectorizer_tokopedia2.sav', 'wb'))
# pickling the model
pickle.dump(classifier_linear, open('classifier_tokopedia2.sav', 'wb'))

"""#### Bukalapak

##### Implementing
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/TUGAS AKHIR/Labeled
data_bukalapak = pd.read_csv('dataset_bukalapak_processed.csv')
print(data_bukalapak.shape)

data_bukalapak.head()

data_bukalapak['clean_msg'] = data_bukalapak['text'].apply(lambda x:remove_punctuation(x))
data_bukalapak['stemmed']= data_bukalapak['clean_msg'].apply(lambda x:stemmer.stem(x))
data_bukalapak['msg_lower']= data_bukalapak['stemmed'].apply(lambda x: x.lower())
data_bukalapak['tokenized']= data_bukalapak.apply(lambda row: nltk.word_tokenize(row['msg_lower']), axis=1)
data_bukalapak['no_stopwords']= data_bukalapak['tokenized'].apply(lambda x:remove_stopwords(x))
data_bukalapak.head()

data_bukalapak.to_csv('dataset_bukalapak_processed.csv')

train_bukalapak, test_bukalapak = train_test_split(data_bukalapak[['text','label','msg_lower','tokenized','no_stopwords']], test_size=0.3, random_state=42)
train_bukalapak.head()

vectorizer_bukalapak = TfidfVectorizer(
    min_df = 5,
    max_df = 0.8,
    sublinear_tf = True,
    use_idf = True)

train_vectors = vectorizer_bukalapak.fit_transform(train_bukalapak['msg_lower'].tolist())
test_vectors = vectorizer_bukalapak.transform(test_bukalapak['msg_lower'].tolist())

import time
from sklearn import svm
from sklearn.metrics import classification_report

# Classification with SVM, kernel=linear
classifier_linear = svm.SVC(kernel='linear')
t0 = time.time()
classifier_linear.fit(train_vectors, train_bukalapak['label'])
t1 = time.time()
prediction_linear = classifier_linear.predict(test_vectors)
t2 = time.time()
time_linear_train = t1-t0
time_linear_predict = t2-t1

# results
print("Training time: %fs; Prediction time: %fs" % (time_linear_train, time_linear_predict))
report = classification_report(test_bukalapak['label'], prediction_linear, output_dict=True)

print('positive: ', report['positif'])
print('negative: ', report['negatif'])

print("Training time: %fs; Prediction time: %fs" % (time_linear_train, time_linear_predict))

"""##### Testing

###### Testing pada Dataset lain | Label : Random
"""

# SHOPEE
from random import randrange
idx = randrange(len(data_test_shopee['content']))

review = data_test_shopee.loc[idx]
review_vector = vectorizer_bukalapak.transform([review['content']])

print('text : ',review['content'])
print('.sentimen : ',classifier_linear.predict(review_vector))

# LAZADA
from random import randrange
idx = randrange(len(data_test_lazada['content']))

review = data_test_lazada.loc[idx]
review_vector = vectorizer_bukalapak.transform([review['content']])

print('text : ',review['content'])
print('.sentimen : ',classifier_linear.predict(review_vector))

# TOKOPEDIA
from random import randrange
idx = randrange(len(data_test_tokopedia['content']))

review = data_test_tokopedia.loc[idx]
review_vector = vectorizer_bukalapak.transform([review['content']])

print('text : ',review['content'])
print('.sentimen : ',classifier_linear.predict(review_vector))

# BUKALAPAK
from random import randrange
idx = randrange(len(data_test_bukalapak['content']))

review = data_test_bukalapak.loc[idx]
review_vector = vectorizer_bukalapak.transform([review['content']])

print('text : ',review['content'])
print('.sentimen : ',classifier_linear.predict(review_vector))

"""##### Saving Models (Pickle)"""

import pickle
# pickling the vectorizer
pickle.dump(vectorizer_bukalapak, open('vectorizer_bukalapak.sav', 'wb'))
# pickling the model
pickle.dump(classifier_linear, open('classifier_bukalapak.sav', 'wb'))

"""## LSTM"""

# NOT YET
#SVM Still need Rework

"""##### Parameter LSTM"""

# DATASET
DATASET_COLUMNS = ["label", "msg_lower"]
DATASET_ENCODING = "ISO-8859-1"
TRAIN_SIZE = 0.7

# TEXT CLENAING
TEXT_CLEANING_RE = "@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"

# WORD2VEC
W2V_SIZE = 300
W2V_WINDOW = 7
W2V_EPOCH = 32
W2V_MIN_COUNT = 10

# KERAS
SEQUENCE_LENGTH = 300
EPOCHS = 32
BATCH_SIZE = 32

# SENTIMENT
POSITIVE = "POSITIVE"
NEGATIVE = "NEGATIVE"
NEUTRAL = "NEUTRAL"
SENTIMENT_THRESHOLDS = (0.4, 0.7)

# EXPORT
KERAS_MODEL = "model.h5"
WORD2VEC_MODEL = "model.w2v"
TOKENIZER_MODEL = "tokenizer.pkl"
ENCODER_MODEL = "encoder.pkl"

"""##### Shopee"""

data_shopee = pd.read_csv('dataset_shopee_processed.csv')
print(data_shopee.isna().sum())
data_shopee.dropna(inplace=True)
print(data_shopee.isna().sum())

labels = Counter(data_shopee.label)
print(labels.keys(), labels.values())

plt.figure(figsize=(16,8))
plt.bar(labels.keys(), labels.values())
plt.title("Dataset labels distribuition shopee")

undersample = pd.concat([data_shopee[data_shopee['label']=='negatif'],
                         data_shopee[data_shopee['label']=='positif'].sample(frac=0.7, random_state=42)])

# now randomly shuffle the dataframe rows in order to mix up the classes again
undersample = undersample.sample(frac=1, random_state=42)

len(undersample)

sample_counter = Counter(undersample.label)
print(sample_counter.keys(), sample_counter.values())

plt.figure(figsize=(16,8))
plt.bar(sample_counter.keys(), sample_counter.values())
plt.title("Dataset labels distribuition Shopee")

train_data, test_data = train_test_split(data_shopee[['label','msg_lower']], test_size=0.2, random_state=42)
print("TRAIN size:", len(train_data))
print("TEST size:", len(test_data))

w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE,
                                            window=W2V_WINDOW,
                                            min_count=W2V_MIN_COUNT,
                                            workers=8)


documents = [_text.split() for _text in train_data.msg_lower]

w2v_model.build_vocab(documents)

words = w2v_model.wv.vocab.keys()
vocab_size = len(words)
print("Vocab size", vocab_size)

w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)

w2v_model.most_similar("lambat")

"""###### Model 1"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_data.msg_lower)

vocab_size = len(tokenizer.word_index) + 1
print("Total words", vocab_size)

from keras_preprocessing.sequence import pad_sequences

x_train = pad_sequences(tokenizer.texts_to_sequences(train_data.msg_lower), maxlen=SEQUENCE_LENGTH)
x_test = pad_sequences(tokenizer.texts_to_sequences(test_data.msg_lower), maxlen=SEQUENCE_LENGTH)

labels = train_data.label.unique().tolist()
labels.append('netral')
labels

encoder = LabelEncoder()
encoder.fit(train_data.label.tolist())

y_train = encoder.transform(train_data.label.tolist())
y_test = encoder.transform(test_data.label.tolist())

y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)

print("y_train",y_train.shape)
print("y_test",y_test.shape)

print("x_train", x_train.shape)
print("y_train", y_train.shape)
print()
print("x_test", x_test.shape)
print("y_test", y_test.shape)

y_test[:10]

embedding_matrix = np.zeros((vocab_size, W2V_SIZE))
for word, i in tokenizer.word_index.items():
  if word in w2v_model.wv:
    embedding_matrix[i] = w2v_model.wv[word]
print(embedding_matrix.shape)

embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)

model2 = Sequential()
model2.add(embedding_layer)
model2.add(Dropout(0.5))
model2.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model2.add(Dense(1, activation='sigmoid'))

model2.summary()

model2.compile(loss='binary_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])

callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),
              EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]

history = model2.fit(x_train, y_train,
                    batch_size=BATCH_SIZE,
                    epochs=16,
                    validation_split=0.1,
                    verbose=1,
                    callbacks=callbacks)

"""###### Evaluate & Predict"""

score = model2.evaluate(x_test, y_test, batch_size=BATCH_SIZE)
print()
print("ACCURACY:",score[1])
print("LOSS:",score[0])

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

def decode_sentiment(score, include_neutral=True):
    if include_neutral:
        label = NEUTRAL
        if score <= SENTIMENT_THRESHOLDS[0]:
            label = NEGATIVE
        elif score >= SENTIMENT_THRESHOLDS[1]:
            label = POSITIVE

        return label
    else:
        return NEGATIVE if score < 0.5 else POSITIVE


def predict(text, include_neutral=True):
    start_at = time.time()
    # Tokenize text
    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)
    # Predict
    score = model2.predict([x_test])[0]
    # Decode sentiment
    label = decode_sentiment(score, include_neutral=include_neutral)

    return {"label": label, "score": float(score),
       "elapsed_time": time.time()-start_at}

predict("MPs vote to reject 12 December election plan - but Boris Johnson is likely to try again for a pre-Christmas poll")

from random import randrange
idx = randrange(len(data_test_shopee['content']))

review = data_test_shopee.loc[idx]
print('text : ',review['content'])
predict(review['content'])

from random import randrange
idx = randrange(len(data_test_lazada['content']))

review = data_test_lazada.loc[idx]
print('text : ',review['content'])
predict(review['content'])

from random import randrange
idx = randrange(len(data_test_tokopedia['content']))

review = data_test_tokopedia.loc[idx]
print('text : ',review['content'])
predict(review['content'])

from random import randrange
idx = randrange(len(data_test_bukalapak['content']))

review = data_test_bukalapak.loc[idx]
print('text : ',review['content'])
predict(review['content'])

"""##### Lazada"""

data_lazada = pd.read_csv('dataset_lazada_processed.csv')
print(data_lazada.isna().sum())
data_lazada.dropna(inplace=True)
print(data_lazada.isna().sum())

labels = Counter(data_lazada.label)
print(labels.keys(), labels.values())

plt.figure(figsize=(16,8))
plt.bar(labels.keys(), labels.values())
plt.title("Dataset labels distribuition Lazada")

undersample = pd.concat([data_lazada[data_lazada['label']=='negatif'],
                         data_lazada[data_lazada['label']=='positif'].sample(frac=0.55, random_state=42)])

# now randomly shuffle the dataframe rows in order to mix up the classes again
undersample = undersample.sample(frac=1, random_state=42)

train_data, test_data = train_test_split(data_lazada[['label','msg_lower']], test_size=0.1, random_state=42)
print("TRAIN size:", len(train_data))
print("TEST size:", len(test_data))

sampled = Counter(train_data.label)
print(sampled.keys(), sampled.values())

plt.figure(figsize=(16,8))
plt.bar(sampled.keys(), sampled.values())
plt.title("Dataset labels distribuition Shopee")

w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE,
                                            window=W2V_WINDOW,
                                            min_count=W2V_MIN_COUNT,
                                            workers=8)


documents = [_text.split() for _text in train_data.msg_lower]

w2v_model.build_vocab(documents)

words = w2v_model.wv.vocab.keys()
vocab_size = len(words)
print("Vocab size", vocab_size)

w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)

w2v_model.most_similar("lambat")

"""###### Model 1"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_data.msg_lower)

vocab_size = len(tokenizer.word_index) + 1
print("Total words", vocab_size)

from keras_preprocessing.sequence import pad_sequences

x_train = pad_sequences(tokenizer.texts_to_sequences(train_data.msg_lower), maxlen=SEQUENCE_LENGTH)
x_test = pad_sequences(tokenizer.texts_to_sequences(test_data.msg_lower), maxlen=SEQUENCE_LENGTH)

labels = train_data.label.unique().tolist()
#labels.append('netral')
labels

encoder = LabelEncoder()
encoder.fit(train_data.label.tolist())

y_train = encoder.transform(train_data.label.tolist())
y_test = encoder.transform(test_data.label.tolist())

y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)

print("y_train",y_train.shape)
print("y_test",y_test.shape)

print("x_train", x_train.shape)
print("y_train", y_train.shape)
print()
print("x_test", x_test.shape)
print("y_test", y_test.shape)

y_test[:10]

embedding_matrix = np.zeros((vocab_size, W2V_SIZE))
for word, i in tokenizer.word_index.items():
  if word in w2v_model.wv:
    embedding_matrix[i] = w2v_model.wv[word]
print(embedding_matrix.shape)

embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)

model_lazada = Sequential()
model_lazada.add(embedding_layer)
model_lazada.add(Dropout(0.5))
model_lazada.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model_lazada.add(Dense(1, activation='sigmoid'))

model_lazada.summary()

model_lazada.compile(loss='binary_crossentropy', #loss='mean_squared_error',
              optimizer="adam",
              metrics=['accuracy'])

callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),
              EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]

history2 = model_lazada.fit(x_train, y_train,
                    batch_size=BATCH_SIZE,
                    epochs=16,
                    validation_split=0.1,
                    verbose=1,
                    callbacks=callbacks)

"""###### Evaluate & Predict"""

score_lazada = model_lazada.evaluate(x_test, y_test, batch_size=BATCH_SIZE)
print()
print("ACCURACY:",score_lazada[1])
print("LOSS:",score_lazada[0])

acc = history2.history['accuracy']
val_acc = history2.history['val_accuracy']
loss = history2.history['loss']
val_loss = history2.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

def decode_sentiment(score, include_neutral=True):
    if include_neutral:
        label = NEUTRAL
        if score <= SENTIMENT_THRESHOLDS[0]:
            label = NEGATIVE
        elif score >= SENTIMENT_THRESHOLDS[1]:
            label = POSITIVE

        return label
    else:
        return NEGATIVE if score < 0.5 else POSITIVE


def predict(text, include_neutral=True):
    start_at = time.time()
    # Tokenize text
    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)
    # Predict
    score = model_lazada.predict([x_test])[0]
    # Decode sentiment
    label = decode_sentiment(score, include_neutral=include_neutral)

    return {"label": label, "score": float(score),
       "elapsed_time": time.time()-start_at}

predict("MPs vote to reject 12 December election plan - but Boris Johnson is likely to try again for a pre-Christmas poll")

from random import randrange
idx = randrange(len(data_lazada['text']))

review = data_lazada.loc[idx]
print('text : ',review['text'])
print('label : ',review['label'])
predict(review['text'])

idx = data_lazada.query("label == 'negatif'").sample(n=1)

rows = np.random.choice(idx.index.values)
review = data_lazada.loc[rows]

print('Text : ',review['text'])
print('Label : ',review['label'])
print('============Predict Score============')
predict(review['text'])

from random import randrange
idx = randrange(len(data_test_shopee['content']))

review = data_test_shopee.loc[idx]
print('text : ',review['content'])
predict(review['content'])

from random import randrange
idx = randrange(len(data_test_lazada['content']))

review = data_test_lazada.loc[idx]
print('text : ',review['content'])
predict(review['content'])

from random import randrange
idx = randrange(len(data_test_tokopedia['content']))

review = data_test_tokopedia.loc[idx]
print('text : ',review['content'])
predict(review['content'])

from random import randrange
idx = randrange(len(data_test_bukalapak['content']))

review = data_test_bukalapak.loc[idx]
print('text : ',review['content'])
predict(review['content'])

"""##### Tokopedia"""

data_tokopedia = pd.read_csv('dataset_tokopedia_processed.csv')
print(data_tokopedia.isna().sum())
data_tokopedia.dropna(inplace=True)
print(data_tokopedia.isna().sum())

labels = Counter(data_tokopedia.label)
print(labels.keys(), labels.values())

plt.figure(figsize=(16,8))
plt.bar(labels.keys(), labels.values())
plt.title("Dataset labels distribuition tokopedia")

undersample = pd.concat([data_tokopedia[data_tokopedia['label']=='negatif'],
                         data_tokopedia[data_tokopedia['label']=='positif'].sample(frac=0.3, random_state=42)])

# now randomly shuffle the dataframe rows in order to mix up the classes again
undersample = undersample.sample(frac=1, random_state=42)

len(undersample)

train_data, test_data = train_test_split(data_tokopedia[['label','msg_lower']], test_size=0.1, random_state=42)
print("TRAIN size:", len(train_data))
print("TEST size:", len(test_data))

sample_counter = Counter(train_data.label)
print(sample_counter.keys(), sample_counter.values())

plt.figure(figsize=(16,8))
plt.bar(sample_counter.keys(), sample_counter.values())
plt.title("Dataset labels distribuition Shopee")

w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE,
                                            window=W2V_WINDOW,
                                            min_count=W2V_MIN_COUNT,
                                            workers=8)


documents = [_text.split() for _text in train_data.msg_lower]

w2v_model.build_vocab(documents)

words = w2v_model.wv.vocab.keys()
vocab_size = len(words)
print("Vocab size", vocab_size)

w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)

w2v_model.most_similar("customer")

"""###### Model 1"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_data.msg_lower)

vocab_size = len(tokenizer.word_index) + 1
print("Total words", vocab_size)

from keras_preprocessing.sequence import pad_sequences

x_train = pad_sequences(tokenizer.texts_to_sequences(train_data.msg_lower), maxlen=SEQUENCE_LENGTH)
x_test = pad_sequences(tokenizer.texts_to_sequences(test_data.msg_lower), maxlen=SEQUENCE_LENGTH)

labels = train_data.label.unique().tolist()
labels.append('netral')
labels

encoder = LabelEncoder()
encoder.fit(train_data.label.tolist())

y_train = encoder.transform(train_data.label.tolist())
y_test = encoder.transform(test_data.label.tolist())

y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)

print("y_train",y_train.shape)
print("y_test",y_test.shape)

print("x_train", x_train.shape)
print("y_train", y_train.shape)
print()
print("x_test", x_test.shape)
print("y_test", y_test.shape)

y_test[:10]

embedding_matrix = np.zeros((vocab_size, W2V_SIZE))
for word, i in tokenizer.word_index.items():
  if word in w2v_model.wv:
    embedding_matrix[i] = w2v_model.wv[word]
print(embedding_matrix.shape)

embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)

model_tokopedia = Sequential()
model_tokopedia.add(embedding_layer)
model_tokopedia.add(Dropout(0.5))
model_tokopedia.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model_tokopedia.add(Dense(1, activation='sigmoid'))

model_tokopedia.summary()

model_tokopedia.compile(loss='binary_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])

callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),
              EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]

history_tokopedia = model_tokopedia.fit(x_train, y_train,
                    batch_size=BATCH_SIZE,
                    epochs=16,
                    validation_split=0.1,
                    verbose=1,
                    callbacks=callbacks)

"""###### Evaluate & Predict"""

score = model_tokopedia.evaluate(x_test, y_test, batch_size=BATCH_SIZE)
print()
print("ACCURACY:",score[1])
print("LOSS:",score[0])

acc = history_tokopedia.history['accuracy']
val_acc = history_tokopedia.history['val_accuracy']
loss = history_tokopedia.history['loss']
val_loss = history_tokopedia.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

def decode_sentiment(score, include_neutral=True):
    if include_neutral:
        label = NEUTRAL
        if score <= SENTIMENT_THRESHOLDS[0]:
            label = NEGATIVE
        elif score >= SENTIMENT_THRESHOLDS[1]:
            label = POSITIVE

        return label
    else:
        return NEGATIVE if score < 0.5 else POSITIVE


def predict(text, include_neutral=True):
    start_at = time.time()
    # Tokenize text
    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)
    # Predict
    score = model_tokopedia.predict([x_test])[0]
    # Decode sentiment
    label = decode_sentiment(score, include_neutral=include_neutral)

    return {"label": label, "score": float(score),
       "elapsed_time": time.time()-start_at}

predict("MPs vote to reject 12 December election plan - but Boris Johnson is likely to try again for a pre-Christmas poll")

from random import randrange
idx = randrange(len(data_tokopedia['text']))

review = data_tokopedia.loc[idx]
print('text : ',review['text'])
print('label : ',review['label'])
predict(review['text'])

idx = data_tokopedia.query("label == 'negatif'").sample(n=1)

rows = np.random.choice(idx.index.values)
review = data_tokopedia.loc[rows]

print('Text : ',review['text'])
print('Label : ',review['label'])
print('============Predict Score============')
predict(review['text'])

# Data Test SHOPEE
from random import randrange
idx = randrange(len(data_test_shopee['content']))

review = data_test_shopee.loc[idx]
print('text : ',review['content'])
predict(review['content'])

# Data Test lazada
from random import randrange
idx = randrange(len(data_test_lazada['content']))

review = data_test_lazada.loc[idx]
print('text : ',review['content'])
predict(review['content'])

# Data Test tokopedia
from random import randrange
idx = randrange(len(data_test_tokopedia['content']))

review = data_test_tokopedia.loc[idx]
print('text : ',review['content'])
predict(review['content'])

# Data Test bukalapak
from random import randrange
idx = randrange(len(data_test_bukalapak['content']))

review = data_test_bukalapak.loc[idx]
print('text : ',review['content'])
predict(review['content'])

"""##### Bukalapak"""

data_bukalapak = pd.read_csv('dataset_bukalapak_processed.csv')
print(data_bukalapak.isna().sum())
data_bukalapak.dropna(inplace=True)
print(data_bukalapak.isna().sum())

labels = Counter(data_bukalapak.label)
print(labels.keys(), labels.values())

plt.figure(figsize=(16,8))
plt.bar(labels.keys(), labels.values())
plt.title("Dataset labels distribuition bukalapak")

train_data, test_data = train_test_split(data_bukalapak[['label','msg_lower']], test_size=0.125, random_state=42)
print("TRAIN size:", len(train_data))
print("TEST size:", len(test_data))

sample_counter = Counter(train_data.label)
print(sample_counter.keys(), sample_counter.values())

plt.figure(figsize=(16,8))
plt.bar(sample_counter.keys(), sample_counter.values())
plt.title("Dataset labels distribuition Shopee")

w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE,
                                            window=W2V_WINDOW,
                                            min_count=W2V_MIN_COUNT,
                                            workers=8)


documents = [_text.split() for _text in train_data.msg_lower]

w2v_model.build_vocab(documents)

words = w2v_model.wv.vocab.keys()
vocab_size = len(words)
print("Vocab size", vocab_size)

w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)

w2v_model.most_similar("jelek")

"""###### Model 1"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_data.msg_lower)

vocab_size = len(tokenizer.word_index) + 1
print("Total words", vocab_size)

from keras_preprocessing.sequence import pad_sequences

x_train = pad_sequences(tokenizer.texts_to_sequences(train_data.msg_lower), maxlen=SEQUENCE_LENGTH)
x_test = pad_sequences(tokenizer.texts_to_sequences(test_data.msg_lower), maxlen=SEQUENCE_LENGTH)

labels = train_data.label.unique().tolist()
labels

encoder = LabelEncoder()
encoder.fit(train_data.label.tolist())

y_train = encoder.transform(train_data.label.tolist())
y_test = encoder.transform(test_data.label.tolist())

y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)

print("y_train",y_train.shape)
print("y_test",y_test.shape)

print("x_train", x_train.shape)
print("y_train", y_train.shape)
print()
print("x_test", x_test.shape)
print("y_test", y_test.shape)

y_test[:10]

embedding_matrix = np.zeros((vocab_size, W2V_SIZE))
for word, i in tokenizer.word_index.items():
  if word in w2v_model.wv:
    embedding_matrix[i] = w2v_model.wv[word]
print(embedding_matrix.shape)

embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)

model_bukalapak = Sequential()
model_bukalapak.add(embedding_layer)
model_bukalapak.add(Dropout(0.5))
model_bukalapak.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))
model_bukalapak.add(Dense(1, activation='sigmoid'))

model_bukalapak.summary()

model_bukalapak.compile(loss='binary_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])

callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),
              EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]

history_bukalapak = model_bukalapak.fit(x_train, y_train,
                    batch_size=BATCH_SIZE,
                    epochs=16,
                    validation_split=0.1,
                    verbose=1,
                    callbacks=callbacks)

"""###### Evaluate & Predict"""

score = model_bukalapak.evaluate(x_test, y_test, batch_size=BATCH_SIZE)
print()
print("ACCURACY:",score[1])
print("LOSS:",score[0])

acc = history_bukalapak.history['accuracy']
val_acc = history_bukalapak.history['val_accuracy']
loss = history_bukalapak.history['loss']
val_loss = history_bukalapak.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

def decode_sentiment(score, include_neutral=False):
    if include_neutral:
        label = NEUTRAL
        if score <= SENTIMENT_THRESHOLDS[0]:
            label = NEGATIVE
        elif score >= SENTIMENT_THRESHOLDS[1]:
            label = POSITIVE

        return label
    else:
        return NEGATIVE if score < 0.5 else POSITIVE


def predict_bukalapak(text, include_neutral=False):
    start_at = time.time()
    # Tokenize text
    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)
    # Predict
    score = model_bukalapak.predict([x_test])[0]
    # Decode sentiment
    label = decode_sentiment(score, include_neutral=include_neutral)

    return {"label": label, "score": float(score),
       "elapsed_time": time.time()-start_at}

predict_bukalapak("MPs vote to reject 12 December election plan - but Boris Johnson is likely to try again for a pre-Christmas poll")

from random import randrange
idx = randrange(len(data_bukalapak['text']))

review = data_bukalapak.loc[idx]
print('text : ',review['text'])
print('label : ',review['label'])
predict_bukalapak(review['text'])

idx = data_bukalapak.query("label == 'negatif'").sample(n=1)

rows = np.random.choice(idx.index.values)
review = data_bukalapak.loc[rows]

print('Text : ',review['text'])
print('Label : ',review['label'])
print('============Predict Score============')
predict_bukalapak(review['text'])

# Data Test shopee
from random import randrange
idx = randrange(len(data_test_shopee['content']))

review = data_test_shopee.loc[idx]
print('text : ',review['content'])
predict(review['content'])

# Data Test lazada
from random import randrange
idx = randrange(len(data_test_lazada['content']))

review = data_test_lazada.loc[idx]
print('text : ',review['content'])
predict(review['content'])

# Data Test tokopedia
from random import randrange
idx = randrange(len(data_test_tokopedia['content']))

review = data_test_tokopedia.loc[idx]
print('text : ',review['content'])
predict(review['content'])

# Data Test bukalapak
from random import randrange
idx = randrange(len(data_test_bukalapak['content']))

review = data_test_bukalapak.loc[idx]
print('text : ',review['content'])
predict(review['content'])

"""##### Test Playground pada data yang di label berdasar rating score"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/TUGAS AKHIR
data_lazada = pd.read_csv('dataset_lazada_cleaned.csv')
print(data_lazada.isna().sum())
data_lazada.dropna(inplace=True)
print(data_lazada.isna().sum())

labels = Counter(data_lazada.label)
print(labels.keys(), labels.values())

plt.figure(figsize=(16,8))
plt.bar(labels.keys(), labels.values())
plt.title("Dataset labels distribuition Lazada")

undersample = pd.concat([data_lazada[data_lazada['label']=='negatif'].sample(n=32500, random_state=42),
                         data_lazada[data_lazada['label']=='positif'].sample(n=32500, random_state=42)])

# now randomly shuffle the dataframe rows in order to mix up the classes again
undersample = undersample.sample(frac=1, random_state=42)

from imblearn.over_sampling import SMOTE

X_resampled, y_resampled = SMOTE().fit_resample(X, y)

sampled = Counter(undersample.label)
print(sampled.keys(), sampled.values())

plt.figure(figsize=(16,8))
plt.bar(sampled.keys(), sampled.values())
plt.title("Dataset labels distribuition Shopee")

train_data, test_data = train_test_split(undersample[['label','msg_lower']], test_size=0.2, random_state=42)
print("TRAIN size:", len(train_data))
print("TEST size:", len(test_data))

w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE,
                                            window=W2V_WINDOW,
                                            min_count=W2V_MIN_COUNT,
                                            workers=8)


documents = [_text.split() for _text in train_data.msg_lower]

w2v_model.build_vocab(documents)

words = w2v_model.wv.vocab.keys()
vocab_size = len(words)
print("Vocab size", vocab_size)

w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)

w2v_model.most_similar("jelek")

"""###### Model 1"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_data.msg_lower)

vocab_size = len(tokenizer.word_index) + 1
print("Total words", vocab_size)

from keras_preprocessing.sequence import pad_sequences

x_train = pad_sequences(tokenizer.texts_to_sequences(train_data.msg_lower), maxlen=SEQUENCE_LENGTH)
x_test = pad_sequences(tokenizer.texts_to_sequences(test_data.msg_lower), maxlen=SEQUENCE_LENGTH)

labels = train_data.label.unique().tolist()
labels.append('netral')
labels

encoder = LabelEncoder()
encoder.fit(train_data.label.tolist())

y_train = encoder.transform(train_data.label.tolist())
y_test = encoder.transform(test_data.label.tolist())

y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)

print("y_train",y_train.shape)
print("y_test",y_test.shape)

print("x_train", x_train.shape)
print("y_train", y_train.shape)
print()
print("x_test", x_test.shape)
print("y_test", y_test.shape)

y_test[:10]

embedding_matrix = np.zeros((vocab_size, W2V_SIZE))
for word, i in tokenizer.word_index.items():
  if word in w2v_model.wv:
    embedding_matrix[i] = w2v_model.wv[word]
print(embedding_matrix.shape)

embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)

model_test = Sequential()
model_test.add(embedding_layer)
model_test.add(Dropout(0.5))
model_test.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
#model_test.add(LSTM(8, dropout=0.2, recurrent_dropout=0.2))
model_test.add(Dropout(0.5))
#model_test.add(Dense(64, activation='relu'))
#model_test.add(Dropout(0.5))
model_test.add(Dense(1, activation='sigmoid'))

model_test.summary()

model_test.compile(loss='binary_crossentropy', #loss='mean_squared_error',
              optimizer="adam",
              metrics=['accuracy'])

callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),
              EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]

history_test = model_test.fit(x_train, y_train,
                    batch_size=BATCH_SIZE,
                    epochs=16,
                    validation_split=0.1,
                    verbose=1,
                    callbacks=callbacks)

"""###### Evaluate & Predict"""

score = model_test.evaluate(x_test, y_test, batch_size=BATCH_SIZE)
print()
print("ACCURACY:",score[1])
print("LOSS:",score[0])

acc = history_test.history['accuracy']
val_acc = history_test.history['val_accuracy']
loss = history_test.history['loss']
val_loss = history_test.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

def decode_sentiment(score, include_neutral=False):
    if include_neutral:
        label = NEUTRAL
        if score <= SENTIMENT_THRESHOLDS[0]:
            label = NEGATIVE
        elif score >= SENTIMENT_THRESHOLDS[1]:
            label = POSITIVE

        return label
    else:
        return NEGATIVE if score < 0.5 else POSITIVE


def predict(text, include_neutral=False):
    start_at = time.time()
    # Tokenize text
    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)
    # Predict
    score = model_test.predict([x_test])[0]
    # Decode sentiment
    label = decode_sentiment(score, include_neutral=include_neutral)

    return {"label": label, "score": float(score),
       "elapsed_time": time.time()-start_at}

predict("MPs vote to reject 12 December election plan - but Boris Johnson is likely to try again for a pre-Christmas poll")

from random import randrange
idx = randrange(len(data_lazada['text']))

review = data_lazada.loc[idx]
print('text : ',review['text'])
print('label : ',review['label'])
predict(review['text'])

idx = data_lazada.query("label == 'negatif'").sample(n=1)

rows = np.random.choice(idx.index.values)
review = data_lazada.loc[rows]

print('Text : ',review['text'])
print('Label : ',review['label'])
print('============Predict Score============')
predict(review['text'])

idx = data_lazada.query("label == 'netral'").sample(n=1)

rows = np.random.choice(idx.index.values)
review = data_lazada.loc[rows]

print('Text : ',review['text'])
print('Label : ',review['label'])
print('============Predict Score============')
predict(review['text'])

idx = data_lazada.query("label == 'netral'").sample(n=1)

rows = np.random.choice(idx.index.values)
review = data_lazada.loc[rows]

print('Text : ',review['text'])
print('Label : ',review['label'])
print('============Predict Score============')
predict(review['text'])

"""#### Test Playground 2"""

from pandas import DataFrame
from pandas import Series
from pandas import concat
from pandas import read_csv
from pandas import datetime
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from math import sqrt
import matplotlib
# be able to save images on server
matplotlib.use('Agg')
from matplotlib import pyplot
import numpy

# date-time parsing function for loading the dataset
def parser(x):
	return datetime.strptime('190'+x, '%Y-%m')

# frame a sequence as a supervised learning problem
def timeseries_to_supervised(data, lag=1):
	df = DataFrame(data)
	columns = [df.shift(i) for i in range(1, lag+1)]
	columns.append(df)
	df = concat(columns, axis=1)
	df = df.drop(0)
	return df

# create a differenced series
def difference(dataset, interval=1):
	diff = list()
	for i in range(interval, len(dataset)):
		value = dataset[i] - dataset[i - interval]
		diff.append(value)
	return Series(diff)

# scale train and test data to [-1, 1]
def scale(train, test):
	# fit scaler
	scaler = MinMaxScaler(feature_range=(-1, 1))
	scaler = scaler.fit(train)
	# transform train
	train = train.reshape(train.shape[0], train.shape[1])
	train_scaled = scaler.transform(train)
	# transform test
	test = test.reshape(test.shape[0], test.shape[1])
	test_scaled = scaler.transform(test)
	return scaler, train_scaled, test_scaled

# inverse scaling for a forecasted value
def invert_scale(scaler, X, yhat):
	new_row = [x for x in X] + [yhat]
	array = numpy.array(new_row)
	array = array.reshape(1, len(array))
	inverted = scaler.inverse_transform(array)
	return inverted[0, -1]

# evaluate the model on a dataset, returns RMSE in transformed units
def evaluate(model, raw_data, scaled_dataset, scaler, offset, batch_size):
	# separate
	X, y = scaled_dataset[:,0:-1], scaled_dataset[:,-1]
	# reshape
	reshaped = X.reshape(len(X), 1, 1)
	# forecast dataset
	output = model.predict(reshaped, batch_size=batch_size)
	# invert data transforms on forecast
	predictions = list()
	for i in range(len(output)):
		yhat = output[i,0]
		# invert scaling
		yhat = invert_scale(scaler, X[i], yhat)
		# invert differencing
		yhat = yhat + raw_data[i]
		# store forecast
		predictions.append(yhat)
	# report performance
	rmse = sqrt(mean_squared_error(raw_data[1:], predictions))
	return rmse

# fit an LSTM network to training data
def fit_lstm(train, test, raw, scaler, batch_size, nb_epoch, neurons):
	X, y = train[:, 0:-1], train[:, -1]
	X = X.reshape(X.shape[0], 1, X.shape[1])
	# prepare model
	model = Sequential()
	model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))
	model.add(Dense(1))
	model.compile(loss='mean_squared_error', optimizer='adam')
	# fit model
	train_rmse, test_rmse = list(), list()
	for i in range(nb_epoch):
		model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)
		model.reset_states()
		# evaluate model on train data
		raw_train = raw[-(len(train)+len(test)+1):-len(test)]
		train_rmse.append(evaluate(model, raw_train, train, scaler, 0, batch_size))
		model.reset_states()
		# evaluate model on test data
		raw_test = raw[-(len(test)+1):]
		test_rmse.append(evaluate(model, raw_test, test, scaler, 0, batch_size))
		model.reset_states()
	history = DataFrame()
	history['train'], history['test'] = train_rmse, test_rmse
	return history

# run diagnostic experiments
def run():
	# load dataset
	series = read_csv('dataset_shopee_cleaned.csv', header=0, parse_dates=[0], index_col=0, squeeze=True)
	# transform data to be stationary
	raw_values = series.values
	diff_values = difference(raw_values, 1)
	# transform data to be supervised learning
	supervised = timeseries_to_supervised(diff_values, 1)
	supervised_values = supervised.values
	# split data into train and test-sets
	train, test = supervised_values[0:-12], supervised_values[-12:]
	# transform the scale of the data
	scaler, train_scaled, test_scaled = scale(train, test)
	# fit and evaluate model
	train_trimmed = train_scaled[2:, :]
	# config
	repeats = 10
	n_batch = 4
	n_epochs = 500
	n_neurons = 1
	# run diagnostic tests
	for i in range(repeats):
		history = fit_lstm(train_trimmed, test_scaled, raw_values, scaler, n_batch, n_epochs, n_neurons)
		pyplot.plot(history['train'], color='blue')
		pyplot.plot(history['test'], color='orange')
		print('%d) TrainRMSE=%f, TestRMSE=%f' % (i, history['train'].iloc[-1], history['test'].iloc[-1]))
	pyplot.savefig('epochs_diagnostic.png')

# entry point
run()



"""###### Model 2"""

lb=LabelEncoder()
undersample['label'] = lb.fit_transform(undersample['label'])

undersample['label'][:10]

tokenizer = Tokenizer(num_words=500, split=' ')
tokenizer.fit_on_texts(undersample['msg_lower'].values)
X = tokenizer.texts_to_sequences(undersample['msg_lower'].values)
X = pad_sequences(X)

model = Sequential()
model.add(Embedding(500, 120, input_length = X.shape[1]))
model.add(SpatialDropout1D(0.4))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(2,activation='softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])
print(model.summary())

#Splitting the data into training and testing
y=pd.get_dummies(undersample['label'])
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)

batch_size=32
history = model.fit(X_train, y_train,
                    epochs = 5,
                    batch_size=batch_size,
                    validation_split=0.1,
                    verbose = 'auto')

model.evaluate(X_test,y_test)

acc = model['accuracy']
val_acc = model['val_accuracy']
loss = model['loss']
val_loss = model['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""##### TESTING"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/TUGAS AKHIR/raw_data
data = pd.read_csv('review_lazada.csv')

data.head()

data.drop(['reviewId','userImage','thumbsUpCount','reviewCreatedVersion','replyContent','repliedAt'], axis=1, inplace=True)
data.head()

data.rename(columns={'content':'text'}, inplace=True)
data.head()

label = []
for i in data.index:
    if data.at[i,'score'] == 1 or data.at[i,'score'] == 2 :
      label.append('negatif')
    elif data.at[i,'score'] == 3 :
      label.append('netral')
    elif data.at[i,'score'] == 4 or data.at[i,'score'] == 5 :
      label.append('positif')

data['label'] = label
data.head()

remove_slangs(data)

data.head()

data['clean_msg']= data['text'].apply(lambda x:remove_punctuation(x))
data['msg_lower']= data['clean_msg'].apply(lambda x: x.lower())
data['stemmed']= data['msg_lower'].apply(lambda x:stemmer.stem(x))
data['tokenized']= data.apply(lambda row: nltk.word_tokenize(row['stemmed']), axis=1)
data['no_stopwords']= data['tokenized'].apply(lambda x:remove_stopwords(x))
data.head()

data.isna().sum()

labels = Counter(data.label)

plt.figure(figsize=(16,8))
plt.bar(labels.keys(), labels.values())
plt.title("Dataset labels distribuition Shopee")